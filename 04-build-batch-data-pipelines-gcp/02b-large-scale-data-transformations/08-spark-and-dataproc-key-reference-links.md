---

## ğŸ“š Key Reference Links for Apache Spark & Google Cloud Dataproc

The following resources are important for understanding **Sparkâ€™s core programming model** and how it is **used effectively on Google Cloud Dataproc** for batch data processing.

---

### ğŸ” Apache Spark â€“ RDD Transformations

ğŸ”— https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations

**What this explains:**
- The foundational Spark RDD programming model
- Common transformations such as `map`, `filter`, `flatMap`, and `reduceByKey`
- How transformations build execution graphs lazily

ğŸ’¡ *This is essential for understanding how Spark processes data in parallel.*

---

### ğŸ—ï¸ Google Cloud Dataproc Templates

ğŸ”— https://docs.cloud.google.com/dataproc/docs/guides/dataproc-templates

**What this helps with:**
- Using pre-built Dataproc templates for common Spark jobs
- Simplifying cluster job submission
- Standardizing batch processing workflows

ğŸ’¡ *Useful for production pipelines where repeatability and speed matter.*

---

## âœ… How These Fit Together

- Learn **Spark transformations** to understand the processing logic  
- Use **Dataproc templates** to run Spark jobs efficiently on Google Cloud  
- Focus on **batch processing patterns**, not cluster management  

Together, these resources help build **scalable, reliable Spark-based batch pipelines** on GCP.

---
